{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "OP8fWnKSiTmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![linear-regression](https://images.spiceworks.com/wp-content/uploads/2022/04/06113401/shutterstock_2087420848.jpg)"
      ],
      "metadata": {
        "id": "GzfFaenIiW01"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj3zrQ5dBUV7"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 2.2: Interpolation**\n",
        "\n",
        "Given a sequence of pairs $\\mathbb{D} = \\{(\\mathbf{x}_i, \\mathbf{y}_i) \\in \\mathbb{R}^d \\times \\mathbb{R}^t : i = 1, \\ldots, n\\}$, we want to find a function $f : \\mathbb{R}^d \\to \\mathbb{R}^t$ so that for all $i$:\n",
        "$$\n",
        "f(\\mathbf{x}_i) \\approx \\mathbf{y}_i.\n",
        "$$\n",
        "Most often, $ t = 1 $, i.e., each $ \\mathbb{y}_i $ is $ real-valued $. However, we will indulge ourselves for treating any $ t $ (since it brings very minimal complication).\n",
        "\n",
        "The variable $ t $ here stands for the number of responses (tasks), i.e., how many values we are interested in predicting (simultaneously).\n",
        "\\\n"
      ],
      "metadata": {
        "id": "U4I7Dk9RBYnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a cool theorem that proves that for any dataset D, we can always find a function f that maps each x to y.\n",
        "You can think of creating a piece wise function for each data point, and leaving the rest 0.\n",
        "Theorem 2.3: Exact interpolation\n",
        "I'll leave the proof here:\n",
        "**Theorem 2.3: Exact interpolation**\n",
        "\n",
        "For any finite number of pairs $ \\mathbb{D} = \\{(\\mathbf{x}_i, y_i) \\in \\mathbb{R}^d \\times \\mathbb{R}^t : i = 1, \\ldots, n\\} $ that satisfy $ x_i = x_j \\implies y_i = y_j $, there exist infinitely many functions $ f : \\mathbb{R}^d \\to \\mathbb{R}^t $ so that for all $ i $:\n",
        "\n",
        "$\n",
        "f(\\mathbf{x}_i) = y_i.\n",
        "$\n",
        "\n",
        "Proof: W.l.o.g. we may assume all $ x_i $'s are distinct. Lagrange polynomials give immediately such a claimed function. More generally, one may put a bump function within a small neighborhood of each $ x_i $ and then glue them together. In details, set $ \\mathbb{N}_i = \\{\\mathbf{z} : \\|\\mathbf{z} - \\mathbf{x}_i\\|_\\infty < \\delta\\} \\subset \\mathbb{R}^d $. Clearly, $ x_i \\in \\mathbb{N}_i $ and for $ \\delta $ sufficiently small, $ \\mathbb{N}_i \\cap \\mathbb{N}_j = \\emptyset $. Define\n",
        "\n",
        "$$\n",
        "f_i(\\mathbf{z}) = \\begin{cases}\n",
        "y_i e^{1/\\delta^3} \\prod_{j=1}^d \\exp \\left( - \\frac{1}{x_i - (x_j - x_i)^+} \\right), & \\text{if } \\mathbf{z} \\in \\mathbb{N}_i \\\\\n",
        "0, & \\text{otherwise}\n",
        "\\end{cases}.\n",
        "$$\n",
        "\n",
        "The function $ f = \\sum_i f_i $ again exactly interpolates our data $ \\mathbb{D} $.\n",
        "\n",
        "The condition $ x_i = x_j \\implies y_i = y_j $ is clearly necessary, for otherwise there cannot be any function so that $ y_i = f(\\mathbf{x}_i) = f(\\mathbf{x}_j) = y_j $ and $ y_i \\neq y_j $. Of course, when all $ x_i $'s are distinct, this condition is trivially satisfied.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ku3oOB1vDkPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a finite training set D, no matter how large\n",
        "its size might be, there exist infinitely many smooth (infinitely differentiable) functions f that maps each xi\n",
        "in D exactly to yi\n",
        ", i.e. they all achieve zero training “error.”\n",
        "But what about a new data point x? , we cannot just map each one to y and leave rest 0.\n",
        "We should find another way.\n"
      ],
      "metadata": {
        "id": "mI7g2aw_ECQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "We will start with the most familiar linear regression, a straight-line fit to data.\n",
        "A straight-line fit is a model of the form:\n",
        "$$\n",
        "y = ax + b\n",
        "$$\n",
        "where $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.\n",
        "\n",
        "Consider the following data, which is scattered about a line with a slope of 2 and an intercept of –5 (see the following figure):"
      ],
      "metadata": {
        "id": "HpNoDH_FC1hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.RandomState(1)\n",
        "x = 10 * rng.rand(50)\n",
        "y = 2 * x - 5 + rng.randn(50)\n",
        "plt.scatter(x, y);"
      ],
      "metadata": {
        "id": "R2UJxrHeCLdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use Scikit-Learn's `LinearRegression` estimator to fit this data and construct the best-fit line, as shown in the following figure:"
      ],
      "metadata": {
        "id": "yHsOGQijDCpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression(fit_intercept=True)\n",
        "\n",
        "model.fit(x[:, np.newaxis], y)\n",
        "\n",
        "xfit = np.linspace(0, 10, 1000)\n",
        "yfit = model.predict(xfit[:, np.newaxis])\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xfit, yfit);"
      ],
      "metadata": {
        "id": "oonqfzJXC_Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition: Least squares regression\n",
        "\n",
        "Our task is to minimize the mean squared error of all data points; that is minimize the expected value of our dataset.\n",
        "This is the least squares regression problem:\n",
        "**Definition 2.6: Least squares regression**\n",
        "\n",
        "To resolve the difficulty in Remark 2.5, we need some statistical assumption on how our data is generated. In particular, we assume $ (X_i, Y_i) $ are independently and identically distributed (i.i.d.) random samples from an unknown distribution $ \\mathbb{P} $. The test sample $ (X, Y) $ is also drawn independently and identically from the same distribution $ \\mathbb{P} $. We are interested in solving the least squares regression problem:\n",
        "\n",
        "$$\n",
        "\\min_{f:\\mathbb{R}^d \\to \\mathbb{R}^t} \\mathbb{E}[\\|Y - f(X)\\|_2^2], \\tag{2.1}\n",
        "$$\n",
        "\n",
        "i.e., finding a function $ f : \\mathbb{R}^d \\to \\mathbb{R}^t $ so that $ f(X) $ approximates $ Y $ well in expectation. (Strictly speaking we need the technical assumption that $ f $ is measurable so that the above expectation is even defined.)\n",
        "\n",
        "In reality, we do not know the distribution $ \\mathbb{P} $ of $ (X, Y) $ hence will not be able to compute the expectation, let alone minimizing it. Instead, we use the training set $ \\mathbb{D} = \\{(X_i, Y_i) : i = 1, \\ldots, n\\} $ to approximate the expectation:\n",
        "\n",
        "$$\n",
        "\\min_{f:\\mathbb{R}^d \\to \\mathbb{R}^t} \\hat{\\mathbb{E}}[\\|Y - f(X)\\|_2^2] := \\frac{1}{n} \\sum_{i=1}^n \\|Y_i - f(X_i)\\|_2^2. \\tag{2.2}\n",
        "$$\n",
        "\n",
        "By law of large numbers, for any fixed function $ f $, we indeed have\n",
        "\n",
        "$$\n",
        "\\frac{1}{n} \\sum_{i=1}^n \\|Y_i - f(X_i)\\|_2^2 \\xrightarrow{n \\to \\infty} \\mathbb{E}[\\|Y - f(X)\\|_2^2].\n",
        "$$\n"
      ],
      "metadata": {
        "id": "gPmj7_ecEPqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 2.11: Linear least squares regression**\n",
        "\n",
        "The simplest choice for the function class $ \\mathbb{F}_n \\equiv \\mathbb{F} $ is perhaps the class of linear/affine functions (recall Definition 1.13). Adopting this choice leads to the linear least squares regression problem:\n",
        "\n",
        "$$\n",
        "\\min_{W \\in \\mathbb{R}^{t \\times d}, b \\in \\mathbb{R}^t} \\frac{1}{n} \\sum_{i=1}^n \\|Y_i - WX_i - b\\|_2^2, \\tag{2.5}\n",
        "$$\n",
        "\n",
        "where recall that $ X_i \\in \\mathbb{R}^d $ and $ Y_i \\in \\mathbb{R}^t $.\n",
        "\n",
        "Rememver here we say F(Xi) = WXi + b\n"
      ],
      "metadata": {
        "id": "Oc9KDAsHFc2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 2.8: Regression function**\n",
        "\n",
        "For a moment let us assume the distribution $ \\mathbb{P} $ is known to us, so we can at least in theory solve the least squares regression problem (2.1). It turns out there exists an optimal solution whose closed-form expression can be derived as follows:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[\\|Y - f(X)\\|_2^2] = \\mathbb{E}[\\|Y - \\mathbb{E}(Y|X) + \\mathbb{E}(Y|X) - f(X)\\|_2^2]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\mathbb{E}[\\|Y - \\mathbb{E}(Y|X)\\|_2^2 + \\mathbb{E}[\\|Y - f(X)\\|_2^2] + 2\\mathbb{E}[(Y - \\mathbb{E}(Y|X), \\mathbb{E}(Y|X) - f(X))]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\mathbb{E}[\\|Y - \\mathbb{E}(Y|X)\\|_2^2 + \\mathbb{E}[\\|Y - f(X)\\|_2^2] + 2\\mathbb{E}[(Y - \\mathbb{E}(Y|X), \\mathbb{E}(Y|X) - f(X))|X]]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\mathbb{E}[\\|Y - \\mathbb{E}(Y|X)\\|_2^2 + \\mathbb{E}[\\mathbb{E}(Y|X) - f(X)\\|_2^2] + 2\\mathbb{E}[(\\mathbb{E}(Y|X) - f(X))]]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\mathbb{E}[\\|Y - \\mathbb{E}(Y|X)\\|_2^2 + \\mathbb{E}[\\mathbb{E}(Y|X) - f(X)\\|_2^2]]\n",
        "$$\n",
        "\n",
        "whence the **regression function**\n",
        "\n",
        "$$\n",
        "m(X) := \\mathbb{E}(Y|X) \\tag{2.3}\n",
        "$$\n",
        "\n",
        "eliminates the second nonnegative term while the first nonnegative term is not affected by any $ f $ at all.\n",
        "\n",
        "With hindsight, it is not surprising the regression function $ m $ is an optimal solution for the least squares regression problem: it basically says given $(X, Y)$, we set $ m(X) = Y $ if there is a unique value of $ Y $ associated with $ X $ (which of course is optimal), while if there are multiple values of $ Y $ associated to the given $ X $, then we simply average them.\n",
        "\n",
        "The constant term $ \\mathbb{E}[\\|Y - \\mathbb{E}(Y|X)\\|_2^2] $ describes the difficulty of our regression problem: no function $ f $ can reduce it.\n",
        "\n",
        "if somoene says he got below $ \\mathbb{E}[\\|Y - \\mathbb{E}(Y|X)\\|_2^2] $, there is something wrong in his implementation, since this value is always >= 0."
      ],
      "metadata": {
        "id": "OZWiBpvCgsDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 2.24: Prediction**\n",
        "\n",
        "Once we have the linear least squares solution $ \\hat{W} = (\\hat{W}, \\hat{b}) $, we perform prediction on a (future) test sample $ X $ naturally by:\n",
        "\n",
        "$$\n",
        "\\hat{Y} := \\hat{W}X + \\hat{b}.\n",
        "$$\n",
        "\n",
        "We measure the “goodness” of our prediction $ \\hat{Y} $ by:\n",
        "\n",
        "$$\n",
        "\\|\\hat{Y} - Y\\|_2^2,\n",
        "$$\n",
        "\n",
        "which is usually averaged over a test set.\n"
      ],
      "metadata": {
        "id": "zoRT-5dTFsu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 2.26: Ridge regression with Tikhonov regularization (Tikhonov63; Hoerl and Kennard 1970)**\n",
        "\n",
        "The class of linear functions may still be too large, leading linear least squares to overfit or be unstable. We can then put some extra restriction, such as the Tikhonov regularization in ridge regression:\n",
        "\n",
        "$$\n",
        "\\min_{W \\in \\mathbb{R}^{t \\times p}} \\frac{1}{n} \\|Y - WX\\|_F^2 + \\lambda \\|W\\|_F^2, \\tag{2.10}\n",
        "$$\n",
        "\n",
        "where $ \\lambda \\ge 0 $ is the regularization constant (hyperparameter) that balances the two terms.\n",
        "\n",
        "To understand ridge regression, consider\n",
        "- when $ \\lambda $ is small, thus we are neglecting the second regularization term, and the solution resembles that of the ordinary linear least squares solution;\n",
        "- when $ \\lambda $ is large, thus we are neglecting the first data term, and the solution degenerates to 0.\n",
        "\n",
        "In the literature, the following variant that chooses not to regularize the bias term $ b $ is also commonly used:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}=[W, b]} \\frac{1}{n} \\|Y - WX\\|_F^2 + \\lambda \\|W\\|_F^2. \\tag{2.11}\n",
        "$$\n",
        "\n",
        "**Reference**:\n",
        "Hoerl, A. E. and R. W. Kennard (1970). \"Ridge regression: Biased estimation for nonorthogonal problems\". Technometrics, vol. 12, no. 1, pp. 55–67.\n"
      ],
      "metadata": {
        "id": "6lc-PebSFwLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical Example:"
      ],
      "metadata": {
        "id": "hxNoOK1zGluU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load the data"
      ],
      "metadata": {
        "id": "qrm8h1dlG7Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://github.com/VIJAY-GADRE/1_Simple_Linear_Regression/blob/main/1.01.%20Simple%20linear%20regression.csv')"
      ],
      "metadata": {
        "id": "vkPiWzB7Gm_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rMXLCNOkHGwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a description about our data\n",
        "# we will compare SAT scores vs GPA\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "Nc9vL5vgHHYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating our first linear regression"
      ],
      "metadata": {
        "id": "-xTUMuoSHVMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we do the following\n",
        "# dataframe[‘column_name’]\n",
        "y = data['GPA']\n",
        "x1 = data['SAT']\n",
        "\n",
        "# then plot: matplotlib.pyplot(independent variable, dependent variable)\n",
        "\n",
        "plt.scatter(x1, y)\n",
        "plt.xlabel('SAT', fontsize = 20)\n",
        "plt.ylabel('GPA', fontsize = 20)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "itFTMagyHR4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform a linear regression we should always add the bias term or the intercept (b0). We can do this using the following method:\n",
        "\n",
        " `statsmodels.add_constant(independent_variable)`"
      ],
      "metadata": {
        "id": "xZX8lCrwHz5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s fit the Linear Regression model using the Ordinary Least Squares (OLS) model with the dependent variable and an independent variable as the arguments."
      ],
      "metadata": {
        "id": "KxGhYfd5H2_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " fit the Linear Regression model using the Ordinary Least Squares (OLS) model with the dependent variable and an independent variable as the arguments.\n",
        " \"\"\"\n",
        "\n",
        "x = sm.add_constant(x1)\n",
        "results = sm.OLS(y, x).fit()\n",
        "results.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "icjqQhemHy4m",
        "outputId": "c729db1a-9a58-42ce-dcfd-687f68487072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n fit the Linear Regression model using the Ordinary Least Squares (OLS) model with the dependent variable and an independent variable as the arguments.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot regression line\n",
        "plt.scatter(x1, y)\n",
        "yhat = 0.0017*x1 + 0.275\n",
        "fig = plt.plot(x1,yhat, lw=4, c='orange', label ='regression line')\n",
        "\n",
        "# add labels\n",
        "plt.xlabel('SAT', fontsize = 20)\n",
        "plt.ylabel('GPA', fontsize = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LR-cIEYiH6n-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}