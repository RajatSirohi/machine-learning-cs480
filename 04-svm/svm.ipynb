{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "qcOsTX6YIAUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a (strictly) linearly separable dataset $ \\mathbb{D} = \\{(\\mathbf{x}_i, y_i) \\subset \\mathbb{R}^d \\times \\{\\pm1\\} : i = 1, \\ldots, n\\} $, there exists a separating hyperplane $ H_{\\mathbf{w}} = \\{\\mathbf{x} \\in \\mathbb{R}^d : (\\mathbf{x}, \\mathbf{w}) + b = 0\\} $, namely that\n",
        "\n",
        "$\n",
        "\\forall i, \\ y_i ((\\mathbf{x}_i, \\mathbf{w}) + b) > 0.\n",
        "$\n",
        "\n",
        "Do you remember this formula from perceptron? (please go back if you don't)\n",
        "We can say that we can always find a place H with weights w that can separate the dataset. We could ask ourselves what is the best plane?\n",
        "\n",
        "In fact, there exist infinitely many separating hyperplanes: if we perturb $(\\mathbf{w}, b)$ slightly, the resulting hyperplane would still be separating, thanks to continuity. Is there a particular separating hyperplane that stands out, and be “optimal”?\n",
        "\n",
        "The answer is yes! Let $ H_{\\mathbb{w}} $ be any separating hyperplane (w.r.t. the given dataset $\\mathbb{D}$). We can compute the distance from each training sample $ \\mathbf{x}_i $ to the hyperplane $ H_{\\mathbf{w}} $:\n",
        "\n",
        "$$\n",
        "\\text{dist}(\\mathbf{x}_i, H_{\\mathbf{w}}) := \\min_{\\mathbf{x} \\in H_{\\mathbf{w}}} \\| \\mathbf{x} - \\mathbf{x}_i \\|_0 \\quad (\\text{e.g., the typical choice } \\| \\cdot \\|_0 = \\| \\cdot \\| = \\| \\cdot \\|_2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\ge \\left| \\frac{\\left< \\mathbf{x} - \\mathbf{x}_i, \\mathbf{w} \\right> + b - b}{\\|\\mathbf{w}\\|} \\right| \\quad (\\text{Cauchy-Schwarz, see Definition 1.25})\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\left| \\frac{\\left< \\mathbf{x}_i, \\mathbf{w} \\right> + b}{\\|\\mathbf{w}\\|} \\right| \\quad (\\text{equality at } \\mathbf{x} = \\mathbf{x}_i - \\frac{\\mathbf{z}}{\\|\\mathbf{w}\\|^2}((\\mathbf{x}_i, \\mathbf{w}) + b), \\left< \\mathbf{z}, \\mathbf{w} \\right> = \\|\\mathbf{w}\\|^2, \\|\\mathbf{z}\\|_0 = \\|\\mathbf{w}\\|)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{y_i ((\\mathbf{x}_i, \\mathbf{w}) + b)}{\\|\\mathbf{w}\\|} \\quad (y_i \\in \\{\\pm 1\\} \\text{ and } H_{\\mathbf{w}} \\text{ is separating}). \\tag{4.1}\n",
        "$$\n",
        "\n",
        "If you're lost, no worries! I was lost too, but what we're doing is basically taking the distance between a point $x_i$ and the hyperplane $H_w$, the shortest distance is always the orthogonal between point $x_i$ and the plane ($x_i$ will touch point $x$ on the plane). Then we give it a lower bound ($\\ge$), and the goal will be to maximize this lower bound. Why lower bound and not the distance itself? Well, most of the time in ML it is easier for us to maximize the lower bound! (ref; ADD LINK).\n",
        "\n",
        "Here and in the following, we always assume w.l.o.g. that the dataset $ \\mathbb{D} $ contains at least 1 positive example and 1 negative example, so that $ \\mathbf{w} = 0 $ with any $ b $ cannot be a separating hyperplane.\n",
        "\n",
        "Among all separating hyperplanes, support vector machines (SVM) tries to find one that maximizes the minimum distance (with the typical choice $ \\| \\cdot \\| = \\| \\cdot \\|_2 $ in mind):\n",
        "\n",
        "$\n",
        "\\max_{\\mathbf{w} : \\forall i, y_i \\hat{y}_i > 0} \\min_{i=1,\\ldots,n} \\frac{y_i \\hat{y}_i}{\\|\\mathbf{w}\\|} \\quad \\text{where} \\quad \\hat{y}_i = (\\mathbf{x}_i, \\mathbf{w}) + b. \\tag{4.2}\n",
        "$\n",
        "We need to find the plane that can take the highest distance (max) of the minimum distance between two points. It needs to be as far as possible.\n"
      ],
      "metadata": {
        "id": "XaBbEwspINbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alert 4.4: Margin as minimum distance**\n",
        "\n",
        "We repeat the formula in Definition 4.2:\n",
        "\n",
        "$$\n",
        "\\text{dist}(\\mathbf{x}, H_{\\mathbf{w}}) := \\left| \\min_{\\mathbf{z} \\in H_{\\mathbf{w}}} \\|\\mathbf{z} - \\mathbf{x}\\|_0 \\right| = \\left| \\frac{(\\mathbf{x}, \\mathbf{w}) + b}{\\|\\mathbf{w}\\|} \\right| = \\frac{y((\\mathbf{x}, \\mathbf{w}) + b)}{\\|\\mathbf{w}\\|} = \\frac{y \\hat{y}}{\\|\\mathbf{w}\\|}\n",
        "$$\n",
        "\n",
        "where the third equality holds if $ y\\hat{y} \\ge 0 $ and $ y \\in \\{\\pm 1\\} $. Given any hyperplane $ H_{\\mathbf{w}} $, we define its margin w.r.t. a data point $(\\mathbf{x}, y)$ as:\n",
        "\n",
        "$$\n",
        "\\gamma((\\mathbf{x}, y); H_{\\mathbf{w}}) := \\frac{y \\hat{y}}{\\|\\mathbf{w}\\|}, \\quad \\hat{y} = (\\mathbf{x}, \\mathbf{w}) + b.\n",
        "$$\n",
        "\n",
        "Geometrically, when the hyperplane $ H_{\\mathbf{w}} $ classifies the data point $(\\mathbf{x}, y)$ correctly (i.e., $ y\\hat{y} > 0 $), this margin is exactly the distance from $ \\mathbf{x} $ to the hyperplane $ H_{\\mathbf{w}} $, and the negation of the distance otherwise.\n",
        "\n",
        "Fixing any hyperplane $ H_{\\mathbf{w}} $, we can extend the notion of its margin to a dataset $ \\mathbb{D} = \\{(\\mathbf{x}_i, y_i) : i = 1, \\ldots, n\\} $ by taking the (worst-case) minimum:\n",
        "\n",
        "$$\n",
        "\\gamma(\\mathbb{D}; H_{\\mathbf{w}}) := \\left| \\min_{i=1,\\ldots,n} \\gamma((\\mathbf{x}_i, y_i); H_{\\mathbf{w}}) \\right| = \\min_i \\frac{y_i \\hat{y}_i}{\\|\\mathbf{w}\\|}, \\quad \\hat{y}_i := (\\mathbf{x}_i, \\mathbf{w}) + b.\n",
        "$$\n",
        "\n",
        "So the goal will be to find the minimum distance then maximize it.\n",
        "\n",
        "Again, when the hyperplane $ H_{\\mathbf{w}} $ (strictly) separates the dataset $ \\mathbb{D} $, the margin $ \\gamma(\\mathbb{D}; H_{\\mathbf{w}}) > 0 $ coincides with the minimum distance, as we saw in Definition 4.2. However, when $ \\mathbb{D} $ is not (strictly) separated by $ H_{\\mathbf{w}} $, the margin $ \\gamma(\\mathbb{D}; H_{\\mathbf{w}}) \\le 0 $ is the negation of the maximum distance among all wrongly classified data points.\n",
        "\n",
        "We can finally define the margin of a dataset $ \\mathbb{D} $ as the (best-case) maximum among all hyperplanes:\n",
        "\n",
        "$\n",
        "\\gamma(\\mathbb{D}) := \\left| \\max_{\\mathbf{w}} \\gamma(\\mathbb{D}; H_{\\mathbf{w}}) \\right| = \\max_{\\mathbf{w}} \\min_{i=1,\\ldots,n} \\frac{y_i \\hat{y}_i}{\\|\\mathbf{w}\\|}. \\tag{4.3}\n",
        "$\n",
        "\n",
        "Again, when the dataset $ \\mathbb{D} $ is (strictly) linearly separable, the margin $ \\gamma(\\mathbb{D}) > 0 $ reduces to the minimum distance to the SVM hyperplane, in which case the margin definition here coincides with what we saw in Remark 1.30 (with the choice $ \\| \\cdot \\|_0 = \\| \\cdot \\| = \\| \\cdot \\|_2 $) and characterizes “how linearly separable” our dataset $ \\mathbb{D} $ is. On the other hand, when $ \\mathbb{D} $ is not (strictly) linearly separable, the margin $ \\gamma(\\mathbb{D}) \\le 0 $.\n",
        "\n",
        "To summarize, hard-margin SVM, as defined in Definition 4.2, maximizes the margin among all hyperplanes on a (strictly) linearly separable dataset. Interestingly, with this interpretation, the hard-margin SVM formulation (4.3) continues to make sense even on a linearly inseparable dataset.\n",
        "\n",
        "In the literature, sometimes people often call the unnormalized quantity $ y\\hat{y} $ margin, which is fine as long as the scale $ \\|\\mathbf{w}\\| $ is kept constant.\n"
      ],
      "metadata": {
        "id": "HUGH3T-LVzx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remark 4.6: Important standardization trick**\n",
        "\n",
        "A simple **standardization** trick in optimization is to introduce an extra variable so that we can reduce an arbitrary objective function to the canonical linear function. For instance, if we are interested in solving:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}} f(\\mathbf{w}),\n",
        "$$\n",
        "\n",
        "where \\( f \\) can be any complicated nonlinear function. Upon introducing an extra variable \\( t \\), we can reformulate our minimization problem equivalently as:\n",
        "\n",
        "$$\n",
        "\\min_{(\\mathbf{w}, t): f(\\mathbf{w}) \\leq t} t,\n",
        "$$\n",
        "\n",
        "where the new objective \\( (0; 1)^\\top (\\mathbf{w}; t) \\) is a simple linear function of \\( (\\mathbf{w}; t) \\). The expense, of course, is that we have to deal with the extra constraint \\( f(\\mathbf{w}) \\leq t \\) now.\n"
      ],
      "metadata": {
        "id": "6MU3LVOHYuF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\max_{\\mathbf{w}:\\|\\mathbf{w}\\|=1} \\min_{i=1,\\ldots,n} y_i \\hat{y}_i. \\tag{4.4}\n",
        "$$\n",
        "\n",
        "Applying the trick in Remark 4.6 (and noting we are maximizing here) yields the reformulation:\n",
        "\n",
        "$$\n",
        "\\max_{(\\mathbf{w}, \\delta):\\|\\mathbf{w}\\|=1} \\delta, \\ \\text{s.t.} \\ \\min_{i=1,\\ldots,n} y_i \\hat{y}_i \\ge \\delta \\ \\Leftrightarrow \\ y_i \\hat{y}_i \\ge \\delta, \\ \\forall i = 1, \\ldots, n,\n",
        "$$\n",
        "\n",
        "which is completely equivalent to (4.3) (except by excluding out the trivial solution \\(\\mathbf{w} = 0\\)).\n",
        "\n",
        "Observe that on any linearly separable dataset, at optimality we can always achieve \\(\\delta \\ge 0\\). Thus, we may relax the unit norm constraint on \\(\\mathbf{w}\\) slightly:\n",
        "\n",
        "$$\n",
        "\\max_{(\\mathbf{w}, \\delta)} \\delta \\ \\text{s.t.} \\ \\|\\mathbf{w}\\| \\le 1, \\ y_i \\hat{y}_i \\ge \\delta, \\ \\forall i = 1, \\ldots, n. \\tag{4.5}\n",
        "$$\n",
        "\n",
        "It is clear if the dataset \\(\\mathbb{D}\\) is indeed linearly separable, at maximum we may choose \\(\\|\\mathbf{w}\\| = 1\\), hence the \"relaxation\" is in fact equivalent (on any linearly separable dataset that consists of at least 1 positive and 1 negative).\n",
        "\n",
        "Rosen, J. (1965). \"Pattern separation by convex programming\". Journal of Mathematical Analysis and Applications, vol. 10, no. 1, pp. 123–134.\n",
        "\n",
        "**Remark 4.10: Removing homogeneity by normalizing offset**\n",
        "\n",
        "A different way to remove the scaling-invariance mentioned in Definition 4.2 is to perform normalization on the offset so that\n",
        "\n",
        "$$\n",
        "\\min_{i=1,\\ldots,n} y_i \\hat{y}_i = \\delta,\n",
        "$$\n",
        "\n",
        "where \\(\\delta > 0\\) is any fixed constant. When the dataset \\(\\mathbb{D}\\) is indeed (strictly) linearly separable, this normalization can always be achieved (simply by scaling \\(\\mathbf{w}\\)). After normalizing this way, we can simplify (4.2) as:\n",
        "\n",
        "$$\n",
        "\\max_{\\mathbf{w}} \\frac{\\delta}{\\|\\mathbf{w}\\|}, \\ \\text{s.t.} \\ \\min_{i=1,\\ldots,n} y_i \\hat{y}_i = \\delta.\n",
        "$$\n",
        "\n",
        "By setting a minimum to the margin which is delta, we are essentially normalizing the margin to a fixed positive value. We can focus on optimizing the hyperplane in a way that ensures all correctly classified points are at least δ away from the hyperplane.\n",
        "\n",
        "This normalization allows for the reformulation of the optimization problem, making it easier to handle mathematically. Instead of dealing with varying distances, we fix the margin and adjust the weights accordingly.\n",
        "\n",
        "So by puttin delta we will ensure that our equation/algo will find a plane such that the margin is at least delta\n",
        "δ. This normalization helps in simplifying the optimization problem.\n",
        "We remind again that \\(\\delta\\) here is any fixed positive constant and we are not optimizing it (in contrast to what we did in Remark 4.7). Applying some elementary transformations (that do not change the minimizer) we arrive at the usual formulation of SVM (due to Boser et al. (1992)):\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\ \\text{s.t.} \\ y_i \\hat{y}_i \\ge \\delta, \\ \\forall i = 1, \\ldots, n. \\tag{4.6}\n",
        "$$\n",
        "\n",
        "It is clear that the actual value of the positive constant \\(\\delta\\) is immaterial. Most often, we simply set \\(\\delta = 1\\), which is our default choice for the rest of this note.\n",
        "\n",
        "The formulation (4.6) only makes sense on (strictly) linearly separable datasets, unlike our original formulation (4.3).\n",
        "\n",
        "Boser, B. E., I. M. Guyon, and V. N. Vapnik (1992). \"A Training Algorithm for Optimal Margin Classifiers\". In: COLT, pp. 144–152.\n"
      ],
      "metadata": {
        "id": "QR4kYzJaYxRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good notebook:\n",
        "https://ampl.com/mo-book/notebooks/05/svm.html"
      ],
      "metadata": {
        "id": "SeesOo2fbNRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Tutorial and Hands-on Notebook\n",
        "\n",
        "This notebook summarizes the concepts of Support Vector Machines (SVM) and provides hands-on examples. We will go through the basic theory, standardization tricks, and how to implement SVM in Python using scikit-learn.\n",
        "\n",
        "## Theory\n",
        "\n",
        "### Basic Concepts\n",
        "\n",
        "1. **Linearly Separable Data**: A dataset is linearly separable if there exists a hyperplane that can separate the data points of different classes without any errors.\n",
        "\n",
        "2. **Hyperplane**: A hyperplane in \\( \\mathbb{R}^d \\) is defined by the equation:\n",
        "   $$\n",
        "   H_{\\mathbf{w}} = \\{\\mathbf{x} \\in \\mathbb{R}^d : (\\mathbf{x}, \\mathbf{w}) + b = 0\\}\n",
        "   $$\n",
        "\n",
        "3. **Margin**: The margin is the distance between the hyperplane and the closest data points from either class. SVM aims to maximize this margin.\n",
        "\n",
        "### Optimization Problem\n",
        "\n",
        "The optimization problem for SVM can be written as:\n",
        "$$\n",
        "\\max_{\\mathbf{w}} \\min_{i=1,\\ldots,n} \\frac{y_i \\hat{y}_i}{\\|\\mathbf{w}\\|}\n",
        "$$\n",
        "where \\( y_i \\) is the label and \\( \\hat{y}_i = (\\mathbf{x}_i, \\mathbf{w}) + b \\).\n",
        "\n",
        "### Standardization Trick\n",
        "\n",
        "To simplify the optimization problem, we can introduce a fixed constant \\( \\delta \\) and reformulate the problem:\n",
        "$$\n",
        "\\min_{i=1,\\ldots,n} y_i \\hat{y}_i = \\delta\n",
        "$$\n",
        "This ensures all points are at least \\( \\delta \\) distance away from the hyperplane.\n",
        "\n",
        "### Reformulated Problem\n",
        "\n",
        "By fixing \\( \\delta \\), we convert the optimization problem to:\n",
        "$$\n",
        "\\max_{(\\mathbf{w}, \\delta):\\|\\mathbf{w}\\|=1} \\delta, \\ \\text{s.t.} \\ \\min_{i=1,\\ldots,n} y_i \\hat{y}_i \\ge \\delta\n",
        "$$\n",
        "\n",
        "## Hands-on Example\n",
        "\n",
        "Let's implement SVM using Python and scikit-learn.\n"
      ],
      "metadata": {
        "id": "lGWsh02CcbEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. Import Libraries\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "-V61T07DcjHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # we will only take the first two features for visualization purposes\n",
        "y = iris.target\n",
        "\n",
        "# make it a binary classification problem\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "X0we1AOFYxtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the SVM model\n",
        "model = SVC(kernel='linear', C=1.0)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Fn61PDZmck-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "b6JDnfLIcmxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the decision boundary\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('SVM Decision Boundary')\n",
        "    plt.show()\n",
        "\n",
        "# plot the decision boundary\n",
        "plot_decision_boundary(model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "eJ4-tDA6coqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more detailed notebook could be found here:\n",
        "https://ampl.com/mo-book/notebooks/05/svm.html"
      ],
      "metadata": {
        "id": "iUa4ks8Cc1Av"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tX5jy5Vsc0jZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}