{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "Axe9FcQMkUCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![log-reg](https://images.spiceworks.com/wp-content/uploads/2022/04/08080448/shutterstock_1917660626.jpg)"
      ],
      "metadata": {
        "id": "bQ-aZ7KZkQkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron Recap with Confidence of prediction**\n",
        "\n",
        "In perceptron we make predictions directly through a linear threshold function:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{sign}(\\mathbf{w}^\\top \\mathbf{x} + b).\n",
        "$$\n",
        "\n",
        "Often, we would also like to know how confident we are about this prediction $\\hat{y}$. For example, we can use the magnitude $\\|\\mathbf{w}^\\top \\mathbf{x} + b\\|$ as the indication of our “confidence.” This choice, however, can be difficult to interpret at times, after all the magnitude could be any positive real number.\n",
        "\n",
        "In the literature there are many attempts to turn the real output of a classifier into probability estimates, see for instance Vovk and Petej (2014) and Vovk et al. (2015).\n",
        "\n"
      ],
      "metadata": {
        "id": "JpvjH9AMI955"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.4: Bernoulli model**\n",
        "\n",
        "Recall that in Bernoulli we have two outputs, e.g., 0 and 1:\n",
        "We can easily model this distribution as such:\n",
        "\n",
        "$$\n",
        "\\Pr(Y = 1|X = \\mathbf{x}) =: p(\\mathbf{x}; \\mathbf{w}),\n",
        "$$\n",
        "\n",
        "this formula makes sense, since we are getting the probability of each point being the true label of it; just come up with an example $x$, give it a label, and $p(x; w)$ should output the estimated probability; while removing the other one (raising to the power 0 is 1).\n",
        "\n",
        "After that, we are going to take the joint distribution of all to get the conditional likelihood:\n",
        "\n",
        "Let us consider the binary classification problem with labels $ y \\in \\{\\pm1\\} $. With the parameterization:\n",
        "\n",
        "$$\n",
        "\\Pr(Y = 1|X = \\mathbf{x}) =: p(\\mathbf{x}; \\mathbf{w}),\n",
        "$$\n",
        "\n",
        "where $ p $ is a function that maps $ \\mathbf{x} $ and $ \\mathbf{w} $ into $ [0, 1] $, we then have the Bernoulli model for generating the label $ y \\in \\{\\pm1\\} $:\n",
        "\n",
        "$$\n",
        "\\Pr(Y = y|X = \\mathbf{x}) = p(\\mathbf{x}; \\mathbf{w})^{(1+y)/2}[1 - p(\\mathbf{x}; \\mathbf{w})]^{(1-y)/2}.\n",
        "$$\n",
        "\n",
        "Let $ \\mathbb{D} = \\{(\\mathbf{x}_i, y_i) : i = 1, \\ldots, n\\} $ be an i.i.d. sample from the same distribution as $ (X, Y) $. The conditional likelihood factorizes under the i.i.d. assumption:\n",
        "\n",
        "$$\n",
        "\\Pr(Y_1 = y_1, \\ldots, Y_n = y_n|X_1 = \\mathbf{x}_1, \\ldots, X_n = \\mathbf{x}_n) = \\prod_{i=1}^n \\Pr(Y_i = y_i|X_i = \\mathbf{x}_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\prod_{i=1}^n p(\\mathbf{x}_i; \\mathbf{w})^{(1+y_i)/2}[1 - p(\\mathbf{x}_i; \\mathbf{w})]^{(1-y_i)/2}. \\tag{3.2}\n",
        "$$\n",
        "\n",
        "A standard algorithm in statistics and machine learning for parameter estimation is to maximize the (conditional) likelihood. In this case, we can maximize (3.2) w.r.t. $ \\mathbf{w} $. Once we figure out $ \\mathbf{w} $, we can then make probability estimates on any new test sample $ \\mathbf{x} $, by simply plugging $ \\mathbf{w} $ and $ \\mathbf{x} $ into (3.1).\n"
      ],
      "metadata": {
        "id": "1BqylRbAK4MM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.6: Logistic Regression:**\n",
        "Let us parametrixe p(x; w); we must choose for it a function that is not too not-too-flexible (does not depend on w) and not-too-inflexible (does not depend on x) (TODO: explain more)\n",
        "\n",
        "Hence, we deifne:\n",
        "$$\n",
        "\\log \\frac{p(\\mathbf{x}; \\mathbf{w})}{1 - p(\\mathbf{x}; \\mathbf{w})} = \\langle \\mathbf{x}, \\mathbf{w} \\rangle + b.\n",
        "$$\n",
        "\n",
        "The ratio on the left-hand side is known as odds ratio (probability of 1 divide by probability of -1). Or\n",
        "equivalently// we then get the sigmoid function !!\n",
        "It is important to note we should not have the label y in our funciton, our job is to predict label y later.\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}; \\mathbf{w}) = \\frac{1}{1 + \\exp(-\\langle \\mathbf{x}, \\mathbf{w} \\rangle - b)} = \\text{sgm}(\\langle \\mathbf{x}, \\mathbf{w} \\rangle + b), \\quad \\text{where} \\quad \\text{sgm}(t) = \\frac{1}{1 + \\exp(-t)} = \\frac{\\exp(t)}{1 + \\exp(t)}\n",
        "$$\n",
        "\n",
        "After pluggig sgm:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}} \\frac{1}{2} \\sum_{i=1}^n (1 + y_i) \\log(1 + \\exp(-\\mathbf{x}_i^\\top \\mathbf{w})) + (1 - y_i) \\log(1 + \\exp(\\mathbf{x}_i^\\top \\mathbf{w})) + (1 - y_i)\\mathbf{x}_i^\\top \\mathbf{w},\n",
        "$$\n",
        "\n",
        "which is usually written in the more succinct form:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}} \\sum_{i=1}^n \\mathbf{1}\\text{gt}(y_i \\hat{y}_i), \\quad \\text{where} \\quad \\hat{y}_i = \\langle \\mathbf{x}_i, \\mathbf{w} \\rangle, \\quad \\text{and} \\quad \\mathbf{1}\\text{gt}(t) := \\log(1 + \\exp(-t))\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "67NbLcpgK4PA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we solve w as in (3.4) and given a new test sample x, we can compute p(x; w) = 1\n",
        "1+exp(−⟨x,w⟩)\n",
        "and\n",
        "predic\n",
        "\n",
        "$$\n",
        "\\hat{y}(\\mathbf{x}) = \\begin{cases}\n",
        "1, & \\text{if} \\ p(\\mathbf{x}; \\mathbf{w}) \\geq 1/2 \\ \\Leftrightarrow \\ \\langle \\mathbf{x}, \\mathbf{w} \\rangle \\geq 0 \\\\\n",
        "-1, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "In other words, for predicting the label we are back to the familiar rule ˆy = sign(⟨x, w⟩). However, now we\n",
        "are also equipped with the probability confidence p(x; w)"
      ],
      "metadata": {
        "id": "pYErkuMFK4RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Logistic regression does more than classification, since it also tries to estimate the posterior probabilities.\n",
        "However, if prediction (of the label) is our sole goal, then we do not have to, and perhaps should not, estimate\n",
        "the posterior probabilities."
      ],
      "metadata": {
        "id": "xLHjnGfnK4T8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm 3.12: Gradient descent for logistic regression**\n",
        "\n",
        "Unlike linear regression, logistic regression no longer admits a closed-form solution. Instead, we can apply gradient descent to iteratively converge to a solution. All we need is to apply the chain rule to compute the gradient of each summand of the objective function in (3.4):\n",
        "This is how we derive the gradient, you could optionally look how:\n",
        "$$\n",
        "\\nabla \\text{lgt}(y_i \\left<\\mathbf{x}_i, \\mathbf{w}\\right>) = -\\frac{\\exp(-t)}{1 + \\exp(-t)} \\bigg|_{t = y_i \\left<\\mathbf{x}_i, \\mathbf{w}\\right>} \\cdot y_i \\mathbf{x}_i = -\\text{sgm}(-y_i \\left<\\mathbf{x}_i, \\mathbf{w}\\right>) \\cdot y_i \\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -y_i \\mathbf{x}_i + \\text{sgm}(y_i \\left<\\mathbf{x}_i, \\mathbf{w}\\right>)y_i \\mathbf{x}_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\left\\{\n",
        "    \\begin{array}{ll}\n",
        "      (p(\\mathbf{x}_i; \\mathbf{w}) - 1)\\mathbf{x}_i, & \\text{if } y_i = 1 \\\\\n",
        "      (p(\\mathbf{x}_i; \\mathbf{w}) - 0)\\mathbf{x}_i, & \\text{if } y_i = -1\n",
        "    \\end{array}\n",
        "  \\right.\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\left(p(\\mathbf{x}_i; \\mathbf{w}) - \\frac{y_i + 1}{2}\\right)\\mathbf{x}_i.\n",
        "$$\n",
        "\n",
        "In the following algorithm, we need to choose a step size $ \\eta $. A safe choice is\n",
        "\n",
        "$$\n",
        "\\eta = \\frac{4}{\\|\\mathbf{X}\\|_2^2},\n",
        "$$\n",
        "\n",
        "namely, the inverse of the largest singular value of the Hessian (see below). An alternative is to start with some small $ \\eta $ and decrease it whenever we are not making progress (known as step size annealing).\n",
        "\n",
        "\\begin{algorithm}[H]\n",
        "\\caption{Gradient descent for binary logistic regression}\n",
        "\\begin{algorithmic}[1]\n",
        "\\Input $ \\mathbf{X} \\in \\mathbb{R}^{d \\times n}, \\mathbf{y} \\in \\{-1, 1\\}^n $ (training set), initialization $ \\mathbf{w} \\in \\mathbb{R}^p $\n",
        "\\Output $ \\mathbf{w} \\in \\mathbb{R}^p $\n",
        "\\For{$ t = 1, 2, \\ldots, \\text{maxiter} $}\n",
        "  \\State Sample a minibatch $ I = \\{i_1, \\ldots, i_m\\} \\subset \\{1, \\ldots, n\\} $\n",
        "  \\State $ \\mathbf{g} \\gets \\mathbf{0} $\n",
        "  \\For{$ i \\in I $}\n",
        "    \\State $ p_i \\gets \\frac{1}{1 + \\exp(-\\left<\\mathbf{x}_i, \\mathbf{w}\\right>)} $ \\hfill // use for-loop only in parallel implementation\n",
        "    \\State $ \\mathbf{g} \\gets \\mathbf{g} + (p_i - \\frac{1 + y_i}{2})\\mathbf{x}_i $ \\hfill // in serial, replace with $ p \\gets \\frac{1}{1 + \\exp(-\\mathbf{X}_{:, I}^\\top \\mathbf{w})} $\n",
        "  \\EndFor\n",
        "  \\State Choose step size $ \\eta > 0 $\n",
        "  \\State $ \\mathbf{w} \\gets \\mathbf{w} - \\eta \\mathbf{g} $\n",
        "  \\State Check stopping criterion\n",
        "\\EndFor\n",
        "\\end{algorithmic}\n",
        "\\end{algorithm}\n",
        "\n",
        "For small problems ($ n \\le 10^4 $ say), we can set $ I = \\{1, \\ldots, n\\} $, i.e., use the entire dataset in every iteration.\n"
      ],
      "metadata": {
        "id": "B__jscDfK4WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remark 3.19: More than 2 classes**\n",
        "\n",
        "We can easily extend logistic regression to \\( c > 2 \\) classes. As before, we make the assumption\n",
        "\n",
        "$$\n",
        "\\Pr(Y = k|X = \\mathbf{x}) = f_k(\\mathbf{W}\\mathbf{x}), \\quad k = 1, \\ldots, c,\n",
        "$$\n",
        "\n",
        "where \\( \\mathbf{W} = [\\mathbf{w}_1, \\ldots, \\mathbf{w}_c]^\\top \\in \\mathbb{R}^{c \\times p} \\) and the vector-valued function \\( f = [f_1, \\ldots, f_c] : \\mathbb{R}^c \\to \\Delta_{c-1} \\) maps a vector of size \\( c \\times 1 \\) to a probability vector in the simplex \\( \\Delta_{c-1} \\). Given an i.i.d. training dataset \\( \\mathcal{D} = \\{(\\mathbf{x}_i, y_i) : i = 1, \\ldots, n\\} \\), where each \\( y_i \\in \\{0, 1\\}^c \\) is a one-hot vector, i.e. \\( \\mathbf{1}^\\top y_i = 1 \\), then the (negated) conditional log-likelihood is:\n",
        "\n",
        "$$\n",
        "- \\log \\Pr(Y_1 = y_1, \\ldots, Y_n = y_n|X_1 = \\mathbf{x}_1, \\ldots, X_n = \\mathbf{x}_n) = - \\sum_{i=1}^n \\sum_{k=1}^c y_{ik} \\log f_k(\\mathbf{W}\\mathbf{x}_i).\n",
        "$$\n"
      ],
      "metadata": {
        "id": "hEPVRWtrT_l-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradients:\n"
      ],
      "metadata": {
        "id": "mQrIOSd8AocM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 2.17: Gradient and Hessian through partial derivatives**\n",
        "\n",
        "Let \\( f : \\mathbb{R}^d \\to \\mathbb{R} \\) be a real-valued smooth function. We can define its gradient through partial derivatives:\n",
        "\n",
        "$$\n",
        "[\\nabla f(\\mathbf{w})]_j = \\frac{\\partial f}{\\partial w_j}(\\mathbf{w}).\n",
        "$$\n",
        "\n",
        "Note that the gradient \\( \\nabla f(\\mathbf{w}) \\in \\mathbb{R}^d \\) has the same size as the input \\( \\mathbf{w} \\).\n",
        "\n",
        "---\n",
        "\n",
        "Similarly, we can define the Hessian through partial derivatives:\n",
        "\n",
        "$$\n",
        "[\\nabla^2 f(\\mathbf{w})]_{ij} = \\frac{\\partial^2 f}{\\partial w_i \\partial w_j}(\\mathbf{w}) = \\frac{\\partial^2 f}{\\partial w_j \\partial w_i}(\\mathbf{w}) = [\\nabla^2 f(\\mathbf{w})]_{ji},\n",
        "$$\n",
        "\n",
        "where the second equality holds as long as \\( f \\) is twice-differentiable. Note that the Hessian is a symmetric matrix \\( \\nabla^2 f(\\mathbf{w}) \\in \\mathbb{R}^{d \\times d} \\) with the same number of rows/columns as the size of the input \\( \\mathbf{w} \\).\n"
      ],
      "metadata": {
        "id": "P1Qy1e2rAoeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, our goal will be to see how the function f changes when w changes, we'll take the partial derivatve since we are talking in the vector space.\n",
        "When coding up: we will take an infintely small change to model this partial:\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial w_j}(\\mathbf{w}) = fi - fi+epsilon / (wi - wi+epsilon).\n",
        "$$\n"
      ],
      "metadata": {
        "id": "hgLNfyx_Aogr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KkSKbHiHAojK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practice Example:\n",
        "\n",
        "# Logistic Regression\n",
        "In logistic regression we perform binary classification of by learnig a function of the form $f_w(x) = \\sigma(x^\\top w)$. Here $x,w \\in \\mathbb{R}^D$, where $D$ is the number of features as before. $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the logistic function.  Let's plot this function below"
      ],
      "metadata": {
        "id": "bZ3T1CBYAolc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "mftYHjsgJI-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic = lambda z: 1./ (1 + np.exp(-z))       #logistic function\n",
        "z = np.linspace(-10,10,100)\n",
        "plt.plot(z, logistic(z))\n",
        "plt.title('logistic function')"
      ],
      "metadata": {
        "id": "uKKB1cL7GtZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic\n",
        "x [N,D]\n",
        "w [D]\n",
        "x@w [N]\n",
        "logistic(x@w) [N]\n",
        "\n",
        "#softmax\n",
        "x [N,D]  R^D->R^C\n",
        "w [D,C]\n",
        "logits = x@w [N,C]\n",
        "logits = logits - np.max(logits, axis=1)\n",
        "softmax[j,i] = exp(logits[j,i])/{np.sum(exp(logits), axis=1)+eps}\n",
        "softmax(x@w) [N,C]"
      ],
      "metadata": {
        "id": "QoIedNC8Gtcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function\n",
        "To fit our model $f_w$ to the data $\\mathbb{D} = \\{x^{(1)}, \\ldots, x^{(N)}\\}$, we maximize the **logarithm of the conditional likelihood**:\n",
        "\n",
        "$$\n",
        "\\ell(w; \\mathcal{D}) = \\sum_n \\log \\mathrm{Bernoulli}(y^{(n)} | \\sigma({x^{(n)}}^\\top w)) = \\sum_n y^{(n)} \\log \\sigma({x^{(n)}}^\\top w)) + (1-y^{(n)}) \\log (1-\\sigma({x^{(n)}}^\\top w)))\n",
        "$$\n",
        "\n",
        "by substituting the definition of logistic function in the equation above, and minimizing the **negative** of the log-likelihood, which is called the **cost function**,\n",
        "we get\n",
        "\n",
        "$$\n",
        "J(w) = \\sum_n y^{(n)} \\log(1+e^{-x w^\\top}) + (1-y^{(n)}) \\log(1+e^{x w^\\top})\n",
        "$$\n",
        "\n",
        "In practice we use mean rather than sum over data points."
      ],
      "metadata": {
        "id": "wCzxh2T5G1iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_fn(x, y, w):\n",
        "    N, D = x.shape\n",
        "    z = np.dot(x, w)\n",
        "    J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z)))  #log1p calculates log(1+x) to remove floating point inaccuracies\n",
        "    return J"
      ],
      "metadata": {
        "id": "a_HnponYGzu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimizing the cost using gradient descent\n",
        "To minimize the cost we use gradient descent: start from some initial assignment to the parameters $w$, and at each iteration take a small step in the opposite direction of the *gradient*. The gradient of the cost function above is given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_d} J(w) =\\sum_n - y^{(n)} x^{(n)}_d \\frac{e^{-w^\\top x^{(n)}}}{1 + e^{-w^\\top x^{(n)}}} +x^{(n)}_d (1- y^{(n)}) \\frac{e^{w^\\top x^{(n)}}}{1 + e^{w^\\top x^{(n)}}} = \\sum_n - x^{(n)}_d y^{(n)} (1-\\hat{y}^{(n)})+ x^{(n)}_d (1- y^{(n)}) \\hat{y}^{(n)} = x^{(n)}_d (\\hat{y}^{(n)} - y^{(n)})\n",
        "$$\n",
        "Since in practice we divide the cost by $N$, we have to the same for the gradient; see the implementation below."
      ],
      "metadata": {
        "id": "BMSvg6m8G8wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(self, x, y):\n",
        "    N,D = x.shape\n",
        "    yh = logistic(np.dot(x, self.w))    # predictions  size N\n",
        "    grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n",
        "    return grad                         # size D"
      ],
      "metadata": {
        "id": "uRB0YSjyG7T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression class\n",
        "Now we are ready to implement the logistic regression class with the usual `fit` and `predict` methods. Here, the `fit` method implements gradient descent."
      ],
      "metadata": {
        "id": "Pjb8XaBpHCoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon                        #to get the tolerance for the norm of gradients\n",
        "        self.max_iters = max_iters                    #maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x,np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf\n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g = self.gradient(x, y)\n",
        "            self.w = self.w - self.learning_rate * g\n",
        "            t += 1\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x,np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x,self.w))            #predict output\n",
        "        return yh\n",
        "\n",
        "LogisticRegression.gradient = gradient             #initialize the gradient method of the LogisticRegression class with gradient function"
      ],
      "metadata": {
        "id": "x08tjSZLHAY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy experiment:\n",
        "fit this linear model to toy data with $x \\in \\Re^1$ + a bias parameter"
      ],
      "metadata": {
        "id": "eAuU_aYKHK7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 50\n",
        "x = np.linspace(-5,5, N)\n",
        "y = ( x < 2).astype(int)                                  #generate synthetic data\n",
        "model = LogisticRegression(verbose=True, )\n",
        "yh = model.fit(x,y).predict(x)\n",
        "plt.plot(x, y, '.', label='dataset')\n",
        "plt.plot(x, yh, 'g', alpha=.5, label='predictions')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel(r'$y$')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDBxL9FXHGSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we see that the model successfully fits the training data. If we run the optimization for long enough the weights will grow large (in absolute value) so as to make the predicted probabilities for the data-points close to the decidion boundary (x=2) close to zero and one.\n"
      ],
      "metadata": {
        "id": "_EhsBqC7HOT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Space\n",
        "Similar to what we did for linear regression, we plot *cost* as a function for logistic regrression as a function of model parameters (weights), and show the correspondence between the different weights having different costs and their fit.\n",
        "The `plot_contour` is the same helper function we used for plotting the cost function for linear regression."
      ],
      "metadata": {
        "id": "cl0BWbpnHRzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "def plot_contour(f, x1bound, x2bound, resolution, ax):\n",
        "    x1range = np.linspace(x1bound[0], x1bound[1], resolution)\n",
        "    x2range = np.linspace(x2bound[0], x2bound[1], resolution)\n",
        "    xg, yg = np.meshgrid(x1range, x2range)\n",
        "    zg = np.zeros_like(xg)\n",
        "    for i,j in itertools.product(range(resolution), range(resolution)):\n",
        "        zg[i,j] = f([xg[i,j], yg[i,j]])\n",
        "    ax.contour(xg, yg, zg, 100)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "nStnZ5E9HPz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define the cost function for linear regression example above, and visualize the cost and the fit of various models (parameters)."
      ],
      "metadata": {
        "id": "4LxSKWraHaI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_plus_bias = np.column_stack([x,np.ones(x.shape[0])])\n",
        "cost_w = lambda param: cost_fn(x_plus_bias, y, param)           #define the cost just as a function of parameters\n",
        "model_list = [(-10, 20), (-2, 2), (3,-3), (4,-4)]\n",
        "fig, axes = plt.subplots(ncols=2, nrows=1, constrained_layout=True, figsize=(10, 5))\n",
        "plot_contour(cost_w, [-50,30], [-10,50],  50, axes[0])\n",
        "colors = ['r','g', 'b', 'k']\n",
        "for i, w in enumerate(model_list):\n",
        "    axes[0].plot(w[0], w[1], 'x'+colors[i])\n",
        "    axes[1].plot(x, y, '.')\n",
        "    axes[1].plot(x, logistic(w[1] + np.dot(w[0], x)), '-'+colors[i], alpha=.5)\n",
        "axes[0].set_xlabel(r'$w_1$')\n",
        "axes[0].set_ylabel(r'$w_0$')\n",
        "axes[0].set_title('weight space')\n",
        "axes[1].set_xlabel('x')\n",
        "axes[1].set_ylabel(r'$y=xw_1 + w_0$')\n",
        "axes[1].set_title('data space')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ng0YnlRJHVlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iris dataset\n",
        "Let's visualize class probabilities for D=2 (plus a bias).\n",
        "To be able to use logistic regression we choose two of the three classes in the Iris dataset."
      ],
      "metadata": {
        "id": "tquvBuWRHb7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "dataset = datasets.load_iris()\n",
        "x, y = dataset['data'][:,:2], dataset['target']\n",
        "x, y = x[y < 2], y[y< 2]                                # we only take the data of class 0 and 1\n",
        "model = LogisticRegression()\n",
        "yh = model.fit(x,y).predict(x)\n",
        "\n",
        "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]), 200)\n",
        "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]), 200)\n",
        "x0,x1 = np.meshgrid(x0v, x1v)\n",
        "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
        "yh_all = model.predict(x_all)\n",
        "plt.scatter(x[:,0], x[:,1], c=yh, marker='o', alpha=1)\n",
        "plt.scatter(x_all[:,0], x_all[:,1], c=yh_all, marker='.', alpha=.05)\n",
        "plt.ylabel('sepal length')\n",
        "plt.xlabel('sepal width')\n",
        "plt.title('class probabilities (colors)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35sEAqwgHdGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/LogisticRegression.ipynb#scrollTo=MTG1cHCdfYC0"
      ],
      "metadata": {
        "id": "g6pkbPBwHou2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQ2B4F6rHptH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}