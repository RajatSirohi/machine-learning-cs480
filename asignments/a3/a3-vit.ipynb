{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eeb0607e",
      "metadata": {
        "id": "eeb0607e"
      },
      "source": [
        "# Assignment 3 (Exercise 1) Vision Transformers (10 pts)\n",
        "\n",
        "In this exercise, we will explore a recent trend in computer vision: Transformers for image recognition. Since [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) successfully applied Transformers to various image recognition benchmarks, numerous follow-up studies have suggested that CNNs may no longer be the go-to architecture for computer vision tasks.\n",
        "\n",
        "In this exercise, we will implement a Vision Transformer and train it on the MNIST dataset. *We recommend completing this assignment using Google Colab with Chrome Browser*.\n",
        "\n",
        "**Submit**\n",
        "1. (<font color='red'>Doc A</font>) Figures and numerical results required in the following questions (include them in the pdf generated by latex file with Exercise 2)\n",
        "2. (<font color='red'>Doc B</font>) The completed *.ipynb file with all the command outputs (can be created by saving the file after finishing the experiment and downloading it from Colab)\n",
        "3. (<font color='red'>Doc C</font>) The PDF version of the saved *.ipynb file (can be created by printing it as a PDF file in Chrome)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1728e3f",
      "metadata": {
        "id": "c1728e3f"
      },
      "source": [
        "## Setup\n",
        "\n",
        "\n",
        "1. In Colab, open tab Runtime > Change runtime type, choose *python3* and *T4 GPU*.\n",
        "2. Run the following command to set up the environment. (Takes ~ 1.5 min)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2bedfbc",
      "metadata": {
        "id": "d2bedfbc"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet \"ipython[notebook]==7.34.0, <8.17.0\" \"setuptools>=68.0.0, <68.3.0\" \"tensorboard\" \"lightning>=2.0.0\" \"urllib3\" \"torch==2.3.0\" \"matplotlib\" \"pytorch-lightning>=1.4, <2.1.0\" \"seaborn\" \"torchvision\" \"torchmetrics>=0.7, <1.3\" \"matplotlib>=3.0.0, <3.9.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc77b35e",
      "metadata": {
        "id": "fc77b35e"
      },
      "source": [
        "<div class=\"center-wrapper\"><div class=\"video-wrapper\"><iframe src=\"https://www.youtube.com/embed/4UyBxlJChfc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div></div>\n",
        "Let's start with importing our standard set of libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50922f13",
      "metadata": {
        "id": "50922f13"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "import lightning as L\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "plt.set_cmap(\"cividis\")\n",
        "%matplotlib inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
        "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
        "sns.reset_orig()\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/VisionTransformers/\")\n",
        "\n",
        "# Setting the seed\n",
        "L.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "if device == torch.device(\"cuda:0\"):\n",
        "  print('Everything looks good; continue')\n",
        "else:\n",
        "  # It is OK if you cannot connect to a GPU. In this case, training the model for\n",
        "  # 5 epoch is sufficient to get full mark.\n",
        "  print('GPU is not detected. Make sure you have chosen the right runtime type')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tLv9gv5VTL4o",
      "metadata": {
        "id": "tLv9gv5VTL4o"
      },
      "source": [
        "## Dataloaders (0 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be381c1",
      "metadata": {
        "id": "8be381c1"
      },
      "source": [
        "We load the MNIST dataset below.\n",
        "The constants in the `transforms.Normalize` correspond to the values\n",
        "that scale and shift the data to a zero mean and standard deviation of\n",
        "one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b38ed9f",
      "metadata": {
        "id": "0b38ed9f"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.1307,], [0.3081,]),\n",
        "    ]\n",
        ")\n",
        "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop((28, 28), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.1307,], [0.3081,]),\n",
        "    ]\n",
        ")\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "# We need to do a little trick because the validation set should not use the augmentation.\n",
        "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
        "val_dataset = MNIST(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
        "L.seed_everything(42)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [55000, 5000])\n",
        "L.seed_everything(42)\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [55000, 5000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = MNIST(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=1)\n",
        "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=1)\n",
        "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=1)\n",
        "\n",
        "# Visualize some examples\n",
        "NUM_IMAGES = 4\n",
        "MNIST_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "MNIST_labels = [val_set[idx][1] for idx in range(NUM_IMAGES)]\n",
        "\n",
        "img_grid = torchvision.utils.make_grid(MNIST_images, nrow=4, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(f\"Image examples of the MNIST dataset (labels: {MNIST_labels})\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b9a82b",
      "metadata": {
        "id": "42b9a82b"
      },
      "source": [
        "## Transformers for image classification (2 pts)\n",
        "\n",
        "Transformers were originally proposed to process sets due to their permutation-equivariant architecture, meaning they produce the same output if the input is permuted. To apply Transformers to sequences, we added positional encoding to the input feature vectors, allowing the model to learn how to use this information effectively. So, why not apply the same approach to images?\n",
        "This is exactly what [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) proposed in their paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" Specifically, the Vision Transformer (ViT) is a model for image classification that views images as sequences of smaller patches.\n",
        "\n",
        "As a preprocessing step, we split an MNIST image of \\(28 x 28\\) pixels into 49 \\(4 x 4\\) patches. Each of these patches is considered a \"word\" or \"token\" and is projected into a feature space. By adding positional encodings and a classification token, we can apply a Transformer to this sequence and start training it for our task.\n",
        "A nice GIF visualization of the architecture is shown below (figure credit - [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/11-vision-transformer/vit_architecture.png\" width=\"600px\"></center>\n",
        "\n",
        "We will walk step by step through the Vision Transformer, and implement all parts by ourselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qaJZhACKXeo1",
      "metadata": {
        "id": "qaJZhACKXeo1"
      },
      "source": [
        "### Split up images\n",
        "An image of size $N\\times N$ has to be split into $(N/M)^2$ patches of size $M\\times M$.\n",
        "\n",
        "These represent the input words to the Transformer.\n",
        "\n",
        "<font color='red'>(Complete the following coding block, 1pt)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3c688d",
      "metadata": {
        "id": "0e3c688d"
      },
      "outputs": [],
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size: Number of pixels per dimension of the patches (integer)\n",
        "        flatten_channels: If True, the patches will be returned in a flattened format\n",
        "                           as a feature vector instead of a image grid.\n",
        "    \"\"\"\n",
        "\n",
        "    # B: batch_size\n",
        "    # C: Num of channels\n",
        "    # H: number of pixels vertically (Height)\n",
        "    # W: number of pixels horizontally (Width)\n",
        "\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    ## Define\n",
        "    # 1) H': number of patches vertically\n",
        "    # 2) W': number of patches horizontally\n",
        "    # 3) p_H: patch's number of pixels vertically\n",
        "    # 4) p_W: patch's number of pixels horizontally\n",
        "\n",
        "    ## Reshape x into [B, H'*W', C, p_H, p_W]\n",
        "\n",
        "    #VVVVVVVVVVV TO BE COMPLETE (START) VVVVVVVVVVVV\n",
        "\n",
        "\n",
        "\n",
        "    # ^^^^^^^^^^^^ TO BE COMPLETE (END) ^^^^^^^^^^^^\n",
        "\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2, 4)  # [B, H'*W', C*p_H*p_W]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d560b31",
      "metadata": {
        "id": "8d560b31"
      },
      "source": [
        "Let's take a look at how that works for our MNIST examples above.\n",
        "For our images of size $28\\times 28$, we choose a patch size of 4.\n",
        "Hence, we obtain sequences of 49 patches of size $4\\times 4$.\n",
        "We visualize them below:\n",
        "\n",
        "\n",
        "<font color='red'>(Run the following command and report the outputted plots in Doc A, 1pt)</font>\n",
        "\n",
        "You should get four plots similar to this:\n",
        "\n",
        "<img src=\"https://github.com/watml/CS480_assignments_imgs/blob/3d4cbbfeea344cfc9f008cb1b7f53d3cea5f10e9/CS480/ass3/demo1.png?raw=true\" alt=\"drawing\" width=\"100\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42cc2515",
      "metadata": {
        "id": "42cc2515"
      },
      "outputs": [],
      "source": [
        "img_patches = img_to_patch(MNIST_images, patch_size=4, flatten_channels=False)\n",
        "\n",
        "fig, ax = plt.subplots(1, MNIST_images.shape[0], figsize=(8, 2))\n",
        "fig.suptitle(\"Images as input sequences of patches\")\n",
        "for i in range(MNIST_images.shape[0]):\n",
        "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=7, normalize=True, pad_value=0.9)\n",
        "    img_grid = img_grid.permute(1, 2, 0)\n",
        "    ax[i].imshow(img_grid)\n",
        "    ax[i].axis(\"off\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce03264",
      "metadata": {
        "id": "fce03264"
      },
      "source": [
        "Compared to the original images, it is much harder to recognize the objects from these patch lists. Still, this is the input we provide to the Transformer for classifying the images. The model must learn how to combine the patches to recognize the objects. The inductive bias in CNNs, that an image is a grid of pixels, is lost in this input format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tw9utEuFZAcU",
      "metadata": {
        "id": "tw9utEuFZAcU"
      },
      "source": [
        "## Building Models  (3 pts)\n",
        "After examining the preprocessing steps, we can now start building the Transformer model. We will use the PyTorch module `nn.MultiheadAttention` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html?highlight=multihead#torch.nn.MultiheadAttention)) to implement the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ez5pjcs_Z1Wp",
      "metadata": {
        "id": "ez5pjcs_Z1Wp"
      },
      "source": [
        "\n",
        "### Attention block\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YfCMguu4aVqB",
      "metadata": {
        "id": "YfCMguu4aVqB"
      },
      "source": [
        "We will use the Pre-Layer Normalization version of the Transformer blocks proposed by [Ruibin Xiong et al.](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf) in 2020. The idea is to apply Layer Normalization not between residual blocks, but instead as the first layer within the residual blocks. This reorganization supports better gradient flow and removes the necessity of a warm-up stage. Below is a visualization of the difference between the standard Post-LN and the Pre-LN versions.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/watml/CS480_assignments_imgs/blob/main/CS480/ass3/demo2.png?raw=true\" width=\"400px\"></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ozgVJ3xj7_Wd",
      "metadata": {
        "id": "ozgVJ3xj7_Wd"
      },
      "source": [
        "\n",
        "\n",
        "<font color='red'>(Follow the plot (b) above to complete the following coding block for the implementation of the Pre-LN Transformer layer, 3pt)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e96682",
      "metadata": {
        "id": "b3e96682"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
        "        \"\"\"Attention Block.\n",
        "\n",
        "        Args:\n",
        "            embed_dim: Dimensionality of input and attention feature vectors\n",
        "            hidden_dim: Dimensionality of hidden layer in feed-forward network\n",
        "                         (usually 2-4x larger than embed_dim)\n",
        "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
        "            dropout: Amount of dropout to apply in the feed-forward network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "        #VVVVVVVVVVV TO BE COMPLETE (START) VVVVVVVVVVVV (1 pt)\n",
        "\n",
        "\n",
        "        # ^^^^^^^^^^^^ TO BE COMPLETE (END) ^^^^^^^^^^^^\n",
        "\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #VVVVVVVVVVV TO BE COMPLETE (START) VVVVVVVVVVVV (2 pt)\n",
        "\n",
        "\n",
        "        # ^^^^^^^^^^^^ TO BE COMPLETE (END) ^^^^^^^^^^^^\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pU13Z4xkbkVw",
      "metadata": {
        "id": "pU13Z4xkbkVw"
      },
      "source": [
        "## Vision Transformer (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389c288f",
      "metadata": {
        "id": "389c288f"
      },
      "source": [
        "Now, we have all the modules to build the Vision Transformer.\n",
        "\n",
        "In addition to the Transformer encoder, we need the following modules:\n",
        "\n",
        "* A **linear projection** layer that maps the input patches to a feature vector of larger size. This is implemented by a simple linear layer that processes each \\( M x M \\) patch independently.\n",
        "* A **classification token** that is added to the input sequence. The output feature vector of this classification token (CLS token) will be used to determine the classification prediction.\n",
        "* Learnable **positional encodings** that are added to the tokens before being processed by the Transformer. These encodings are necessary to capture position-dependent information and convert the set of patches into a sequence. Since we typically work with a fixed resolution, we can learn the positional encodings instead of using predefined sine and cosine functions.\n",
        "* An **MLP head** that takes the output feature vector of the CLS token and maps it to a classification prediction. This is usually implemented as a small feed-forward network or even a single linear layer.\n",
        "\n",
        "\n",
        "<font color='red'>(Complete the following coding block, 2 pts)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8000482",
      "metadata": {
        "id": "d8000482"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        num_channels,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        patch_size,\n",
        "        num_patches,\n",
        "        dropout=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Vision Transformer.\n",
        "\n",
        "        Args:\n",
        "            embed_dim: Dimensionality of the input feature vectors for the Transformer.\n",
        "            hidden_dim: Dimensionality of the hidden layer in the Transformer's feed-forward networks.\n",
        "            num_channels: Number of channels in the input (e.g., 1 for grayscale, 3 for RGB).\n",
        "            num_heads: Number of heads in the Multi-Head Attention block.\n",
        "            num_layers: Number of layers in the Transformer.\n",
        "            num_classes: Number of classes for classification.\n",
        "            patch_size: Size of each patch in pixels per dimension.\n",
        "            num_patches: Maximum number of patches an image can contain.\n",
        "            dropout: Dropout rate for the feed-forward network and the input encoding.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Layers/Networks\n",
        "        self.input_layer = nn.Linear(num_channels * (patch_size**2), embed_dim)\n",
        "        self.transformer = nn.Sequential(\n",
        "            *(AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers))\n",
        "        )\n",
        "        self.mlp_head = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Parameters/Embeddings\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Preprocess input\n",
        "        x = img_to_patch(x, self.patch_size)\n",
        "        B, T, _ = x.shape\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "\n",
        "\n",
        "        #VVVVVVVVVVV TO BE COMPLETE (START) VVVVVVVVVVVV\n",
        "        # Add CLS token and positional encoding\n",
        "\n",
        "        # 1. concatenate cls_token to all pixels (1 pt)\n",
        "\n",
        "\n",
        "        # 2. add pos_embedding (1 pt)\n",
        "\n",
        "\n",
        "        # ^^^^^^^^^^^^ TO BE COMPLETE (END) ^^^^^^^^^^^^\n",
        "\n",
        "\n",
        "        # Apply Transforrmer\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Perform classification prediction\n",
        "        cls = x[0]\n",
        "        out = self.mlp_head(cls)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c_2IEKYZYzhN",
      "metadata": {
        "id": "c_2IEKYZYzhN"
      },
      "source": [
        "## Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbbbec8f",
      "metadata": {
        "id": "bbbbec8f"
      },
      "source": [
        "Finally, we can put everything into a [PyTorch Lightning Module](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html).\n",
        "\n",
        "PyTorch Lightning is a high-level framework built on top of PyTorch that is designed to streamline the process of building and training complex deep learning models. PyTorch Lightning automates common training tasks such as checkpointing, logging, and GPU/TPU training, reducing the need for repetitive code. Check this 15-minute [intro video](https://lightning.ai/docs/pytorch/stable/starter/introduction.html) for more information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We use `torch.optim.AdamW` as the optimizer, which is Adam with a corrected weight decay implementation.\n",
        "Since we use the Pre-LN Transformer version, we do not need to use a learning rate warmup stage anymore.\n",
        "Instead, we use the same learning rate scheduler as the CNNs in the previous assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c23f32f8",
      "metadata": {
        "id": "c23f32f8"
      },
      "outputs": [],
      "source": [
        "class ViT(L.LightningModule):\n",
        "    def __init__(self, model_kwargs, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = VisionTransformer(**model_kwargs)\n",
        "        self.example_input_array = next(iter(train_loader))[0]  # For plotting the computation graph\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        return [optimizer], []\n",
        "\n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6db3bc8",
      "metadata": {
        "id": "d6db3bc8"
      },
      "source": [
        "## Experiments (3 pts)\n",
        "\n",
        "Commonly, Vision Transformers are used for large-scale image classification tasks such as ImageNet to fully leverage their capabilities. However, for demonstration purposes, we are training a Vision Transformer from scratch on the MNIST dataset.\n",
        "\n",
        "\n",
        "Let's start by creating a training function for our PyTorch Lightning module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c340d778",
      "metadata": {
        "id": "c340d778"
      },
      "outputs": [],
      "source": [
        "def train_model(**kwargs):\n",
        "    trainer = L.Trainer(\n",
        "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        max_epochs=10,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "            LearningRateMonitor(\"epoch\"),\n",
        "        ],\n",
        "    )\n",
        "    trainer.logger._log_graph = True  # Enable computation graph plotting in TensorBoard\n",
        "    trainer.logger._default_hp_metric = None  # Disable default hyperparameter logging\n",
        "\n",
        "    # Check for an existing pretrained model\n",
        "    pretrained_path = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
        "    if os.path.isfile(pretrained_path):\n",
        "        print(f\"Found pretrained model at {pretrained_path}, loading...\")\n",
        "        model = ViT.load_from_checkpoint(pretrained_path)\n",
        "    else:\n",
        "        L.seed_everything(42)  # Ensure reproducibility\n",
        "        model = ViT(**kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        # Load the best checkpoint after training\n",
        "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Evaluate the best model on validation and test sets\n",
        "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
        "    results = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0209f85a",
      "metadata": {
        "id": "0209f85a"
      },
      "source": [
        "Start [tensorboard](https://www.tensorflow.org/tensorboard) to monitor the training process.\n",
        "\n",
        "(**Note:** a webpage will show up once you run the following command. Press refresh button on the top right corner to refresh or click the gear button to enable automatic refresh)\n",
        "\n",
        "(**Note:** All logs are cached unless you restart the runtime environment. If the webpage is stuck or missing some tabs, you can re-run the following command without losing any logged results.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a883635e",
      "metadata": {
        "id": "a883635e"
      },
      "outputs": [],
      "source": [
        "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/saved_models/VisionTransformers/ViT/lightning_logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8950685",
      "metadata": {
        "id": "d8950685"
      },
      "source": [
        "Now, we can start training our model.\n",
        "\n",
        "Feel free to explore the hyperparameters by adjusting the values below. Generally, the Vision Transformer has not shown to be overly sensitive to hyperparameter choices on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bdd2011",
      "metadata": {
        "id": "4bdd2011"
      },
      "outputs": [],
      "source": [
        "model, results = train_model(\n",
        "    model_kwargs={\n",
        "        \"embed_dim\": 64,\n",
        "        \"hidden_dim\": 128,\n",
        "        \"num_heads\": 4,\n",
        "        \"num_layers\": 3,\n",
        "        \"patch_size\": 4,\n",
        "        \"num_channels\": 1,\n",
        "        \"num_patches\": 64,\n",
        "        \"num_classes\": 10,\n",
        "        \"dropout\": 0.1,\n",
        "    },\n",
        "    lr=3e-4,\n",
        ")\n",
        "print(\"ViT results\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bh9mvVkiFemm",
      "metadata": {
        "id": "Bh9mvVkiFemm"
      },
      "source": [
        "<font color='red'> Report the test accuracy in Doc A. (1 pt)</font>\n",
        "\n",
        "<font color='red'> Take a screenshot from tensorboard for train_acc vs epochs and include it in Doc A. (1 pt)</font>\n",
        "\n",
        "<font color='red'> Take a screenshot from tensorboard for val_acc vs epochs and include it in Doc A. (1 pt)</font>\n",
        "\n",
        "(**Note**: The last plots can be found in **SCALARS**. If necessary, rerun the tensorboard-related commands after the training is completed.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7465d825",
      "metadata": {
        "id": "7465d825"
      },
      "source": [
        "## References\n",
        "\n",
        "Dosovitskiy, Alexey, et al.\n",
        "\"An image is worth 16x16 words: Transformers for image recognition at scale.\"\n",
        "International Conference on Representation Learning (2021).\n",
        "[link](https://arxiv.org/pdf/2010.11929.pdf)\n",
        "\n",
        "Chen, Xiangning, et al.\n",
        "\"When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.\"\n",
        "arXiv preprint arXiv:2106.01548 (2021).\n",
        "[link](https://arxiv.org/abs/2106.01548)\n",
        "\n",
        "Tolstikhin, Ilya, et al.\n",
        "\"MLP-mixer: An all-MLP Architecture for Vision.\"\n",
        "arXiv preprint arXiv:2105.01601 (2021).\n",
        "[link](https://arxiv.org/abs/2105.01601)\n",
        "\n",
        "Xiong, Ruibin, et al.\n",
        "\"On layer normalization in the transformer architecture.\"\n",
        "International Conference on Machine Learning.\n",
        "PMLR, 2020.\n",
        "[link](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "id,colab,colab_type,-all",
      "formats": "ipynb,py:percent",
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 115.589197,
      "end_time": "2023-10-11T16:50:18.231450",
      "environment_variables": {},
      "exception": null,
      "input_path": "course_UvA-DL/11-vision-transformer/Vision_Transformer.ipynb",
      "output_path": ".notebooks/course_UvA-DL/11-vision-transformer.ipynb",
      "parameters": {},
      "start_time": "2023-10-11T16:48:22.642253",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}